import pandas as pd
import jieba
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn_extra.cluster import KMedoids
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from collections import Counter

# ========= 1. è®€å–è³‡æ–™ ===============
df = pd.read_csv('youtube_multi_video_comments.csv')
df = df.dropna(subset=['text'])

# ========= 2. ä¸­æ–‡æ¸…æ´—èˆ‡æ–·è© ===============
def clean_and_cut(text):
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', '', str(text))
    words = jieba.cut(text)
    return ' '.join(words)

df['cleaned_text'] = df['text'].apply(clean_and_cut)

# ========= 3. å‘é‡åŒ–ï¼ˆTF-IDFï¼‰ ===============
vectorizer = TfidfVectorizer(max_features=1000)
X = vectorizer.fit_transform(df['cleaned_text'])

# ========= 4. åˆ†ç¾¤ï¼ˆK-Medoidsï¼‰ ===============
k = 4  # æƒ³åˆ†å¹¾ç¾¤å°±æ”¹é€™è£¡
medoids = KMedoids(n_clusters=k, metric='cosine', random_state=42)
df['cluster'] = medoids.fit_predict(X)

# ========= 5. åˆ†ç¾¤é—œéµè©çµ±è¨ˆ ===============
print("ğŸ“Œ å„ç¾¤ Top 10 é—œéµè©ï¼ˆK-Medoidsï¼‰ï¼š\n")
for i in range(k):
    cluster_texts = df[df['cluster'] == i]['cleaned_text']
    words = ' '.join(cluster_texts).split()
    top_words = Counter(words).most_common(10)
    print(f'ğŸ”¹ Cluster {i}: {top_words}')

# ========= 6. é™ç¶­ + è¦–è¦ºåŒ– ===============
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X.toarray())

plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['cluster'], cmap='tab10', alpha=0.7)
plt.title('ç•™è¨€å…§å®¹åˆ†ç¾¤è¦–è¦ºåŒ–ï¼ˆK-Medoids + PCAï¼‰')
plt.xlabel('PCA-1')
plt.ylabel('PCA-2')
plt.grid(True)
plt.show()
